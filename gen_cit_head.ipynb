{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import distance\n",
    "import re\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import xml.etree.cElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Functions to normalize string\n",
    "def to_normal(a1):\n",
    "    a1 = re.sub(r'\\d+', '', a1)    # removes int\n",
    "    a1 = re.sub(r'\\W+', '', a1)    # removes non-alphanumeric\n",
    "    #a1 = re.sub(r'[^\\w\\s]','',a1)  # removes non-alphanumeric(retains whitespaces)\n",
    "    a1 = re.sub('\\n', '', a1)\n",
    "    a1 = a1.strip()\n",
    "    a1 = a1.lower()\n",
    "    return a1\n",
    "\n",
    "#Function to find match for str(a1) in list_of_strings(a2)\n",
    "def find_match(a1,a2):\n",
    "    match_limit = 0.1\n",
    "    a1 = to_normal(a1)\n",
    "    dis = 1\n",
    "    paper = None\n",
    "    \n",
    "    for elem in a2: \n",
    "        #Not applying to_normal to elem in a2(will make code very slow)\n",
    "        #to_normal must be applied to every element of a2 before passing in the function \n",
    "    \n",
    "        score = distance.nlevenshtein(a1, elem)\n",
    "        if score<dis:\n",
    "            paper = elem\n",
    "            dis = score\n",
    "    if dis<match_limit:\n",
    "        return paper\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading 'network.csv' .......\n",
    "print '.....Loading csv file of network .......'\n",
    "network = pd.read_csv(\"network.csv\")\n",
    "\n",
    "#Creating incite and outcite dictionary\n",
    "print '.....Creating incite and outcite dictionary.....'\n",
    "incite = {}\n",
    "outcite = {}\n",
    "for row in network.iterrows():\n",
    "    if(type(row[1][1])!=str):\n",
    "        incite[row[1][0]] = []\n",
    "    else:\n",
    "        incite[row[1][0]] = row[1][1].split(',')\n",
    "    if(type(row[1][2])!=str):\n",
    "        outcite[row[1][0]] = []\n",
    "    else:\n",
    "        outcite[row[1][0]] = row[1][2].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Loading/Creating paper_array \n",
    "## paper_array = names of all papers in dataset ordered lexically\n",
    "print '.....Loading/Creating paper_array..... '\n",
    "\n",
    "paper_dir = \"../2014/papers_text\"\n",
    "paper_array_path = 'pickled/paper_array.txt'\n",
    "\n",
    "if os.path.isfile(paper_array_path):\n",
    "    with open(paper_array_path, \"rb\") as array_file:\n",
    "        paper_array = pickle.load(array_file)\n",
    "else:\n",
    "    paper_array = []\n",
    "    for filename in os.listdir(paper_dir):\n",
    "        if filename.endswith(\".txt\"): \n",
    "            paper_array.append(filename[:-4])\n",
    "    paper_array = sorted(paper_array)\n",
    "    with open(paper_array_path, \"wb\") as array_file:\n",
    "        pickle.dump(paper_array, array_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate dict from paper_ids.txt \n",
    "print '.....Generating dict from paper_ids.txt ..... '\n",
    "paper_ids=open(\"../2014/paper_ids.txt\",\"r\")\n",
    "dict_id={}\n",
    "for line in paper_ids:\n",
    "    key=line[:8]\n",
    "    value=to_normal(line[9:-5].strip())\n",
    "    dict_id[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initializing outcite_1 aka. dict_cit_head.....\n",
    "outcite_1 = {}\n",
    "for paper_id in paper_array:\n",
    "    outcite_1[paper_id] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....building dict_cit_head .....\n",
      "---- time taken for last batch 0--to--99 = 0.00327801704407\n"
     ]
    }
   ],
   "source": [
    "## Generating outcite_1 aka. dict_cit_head.....\n",
    "print '.....building dict_cit_head .....'\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "loop_count = -1\n",
    "for curr_paper in outcite_1.keys()[:10]:\n",
    "    loop_count+=1\n",
    "    doc_name = '../xml/'+curr_paper[:3]+'/'+curr_paper+'-parscit.130908'+'.xml'\n",
    "    if loop_count%100 ==0:\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print '---- time taken for last batch '+str(loop_count-99)+'--to--'+str(loop_count)+' = ' +str(elapsed)\n",
    "    if os.path.isfile(doc_name):\n",
    "        tree = ET.ElementTree(file= doc_name)\n",
    "        root = tree.getroot()\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    heading = []\n",
    "    body = []\n",
    "    count = -1\n",
    "    for elem in root[0][0]:\n",
    "        count+=1\n",
    "        if elem.tag == 'sectionHeader' or elem.tag =='subsectionHeader' or elem.tag == 'subsubsectionHeader':\n",
    "            heading.append([count, elem.text])\n",
    "        if elem.tag == 'bodyText':\n",
    "            body.append([count, elem.text])\n",
    "        \n",
    "#print heading[:9]\n",
    "#print body[1][1]\n",
    "\n",
    "    cit_head_map = {}\n",
    "    for elem in tree.iter(tag='citation'):\n",
    "        heads_ind = []\n",
    "        citstr = []\n",
    "        title = []\n",
    "        body_ind = []\n",
    "        for child in elem:\n",
    "            if child.tag == 'title':\n",
    "                title = child.text       \n",
    "            if child.tag == 'contexts':            \n",
    "                citstr = child[0].attrib['citStr']\n",
    "            \n",
    "        if (citstr == []) or (title == []):\n",
    "            continue\n",
    "        else:\n",
    "        #print 'title: '+ title\n",
    "        #print 'citStr: '+ citstr\n",
    "    \n",
    "            for entry in body:\n",
    "                if citstr in entry[1]:\n",
    "                    body_ind.append(entry[0])\n",
    "            \n",
    "            if len(body_ind)!=0:\n",
    "                for ind in body_ind:\n",
    "                    #print 'body: '+ str(ind)\n",
    "                    sect_ind = -1\n",
    "                    for entry in heading:\n",
    "                        if entry[0]<ind and entry[0]>sect_ind:\n",
    "                            sect_ind = entry[0]\n",
    "                    #print sect_ind\n",
    "                    sect = root[0][0][sect_ind].text\n",
    "                    sect = to_normal(sect)\n",
    "                    #print 'sect:'+ sect\n",
    "                    cit_head_map[to_normal(title)]=sect.strip()\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    for entry in outcite[curr_paper]:\n",
    "        #print entry\n",
    "        matched = find_match(dict_id[entry],cit_head_map.keys())\n",
    "        if matched:\n",
    "            #print 'match_found !'\n",
    "            outcite_1[curr_paper].append([entry,cit_head_map[matched]])\n",
    "    if loop_count%100 == 0:\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....saving dict_cit_head in pickled.....\n",
      ".....saved dict_cit_head in pickled.....\n"
     ]
    }
   ],
   "source": [
    "# Saving dict_cit_head\n",
    "print '....saving dict_cit_head in pickled.....'\n",
    "with open(\"pickled/dict_cit_head.txt\", \"wb\") as dict_file:\n",
    "    pickle.dump(outcite_1, dict_file)\n",
    "print '.....saved dict_cit_head in pickled.....'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Code Ends Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
